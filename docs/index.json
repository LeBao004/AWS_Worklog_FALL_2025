[{"uri":"https://lebao004.github.io/AWS_Worklog_2025/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: LE HO GIA BAO\nPhone Number: 0398348387\nEmail: baolhgse184518@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: ‚ÄúAI-powered planning, design, and coding for modern software development‚Äù Overview: An on-demand webinar from AWS Marketplace focused on bringing Generative AI into planning ‚Äì design ‚Äì coding across the SDLC to accelerate collaboration, automate testing, generate docs/diagrams, and enforce security via a zero-trust approach. (Source: AWS Marketplace webinar page ‚Äì On-demand).\nEvent Objectives Explain how GenAI reshapes planning, design, and coding on AWS. Show how to integrate GenAI into Agile/Scrum: sprint planning, backlog refinement, test generation. Demonstrate creating UI/UX mock-ups, architecture diagrams, and technical docs with AI for faster alignment. Share security/zero-trust practices for code analysis and architecture reviews. Speakers Harrison Kirby ‚Äî Ambassador, DevOps Institute Ronak Shah ‚Äî Principal Solutions Architect, AWS Highlights 1) GenAI across the SDLC Planning \u0026amp; Design: AI proposes architecture options and generates diagrams/docs; supports early decision-making. Coding \u0026amp; Testing: Real-time code suggestions, unit/integration test generation, fewer fix-rework cycles. Collaboration: Rapid UI/UX mock-ups to align architects, developers, and designers. 2) ‚ÄúAttendees will learn‚Äù (from the webinar page) Embedding GenAI into Agile workflows for sprint planning, backlog refinement, test generation. Using AI-generated visuals/diagrams/code to accelerate cross-functional collaboration. Applying security frameworks and zero-trust principles with AI for architecture reviews and code analysis. 3) Practical tie-ins (from your supporting document) Governance Copilot: flags scope creep/budget drift; auto-creates minutes and risk registers. Smarter Estimation: learns from historical projects; outputs best/worst-case ranges, not a single point. Scope Clarifier: NLP capture/analysis to detect ambiguous/conflicting/missing requirements before scope lock. Dependency Radar: AI-graph mapping of team/vendor/module dependencies to avoid bottlenecks. Auto-documentation: keeps UML/sequence/workflow/API docs in sync with code/design changes. Key Takeaways Vision ‚Üí Value: anchor every GenAI effort to clear KPIs/ROI (speed, cost, quality). Data-first: retrieval/embedding/rerank quality drives output quality. Security-by-design: zero-trust, access control, PII protection, content moderation, cost/token visibility. Observability \u0026amp; Eval: tracing, online/offline evaluation, continuous feedback loops. Applying to Work Pilot 1‚Äì2 GenAI use cases over 6‚Äì8 weeks with go/no-go gates (quality, latency, cost/interaction, adoption). Enable a governance copilot (scope/budget alerts) and auto-documentation from the first sprint. Standardize estimations with historical data; publish 2‚Äì3 scenarios instead of one number. Use a scope clarifier for all requirement sessions; run a dependency radar before major design milestones. Event Experience The webinar shows how to operationalize AI‚Äîfrom documentation/architecture to code/test and security‚Äîhelping teams reduce process friction and shorten lead time while maintaining safety and scalability on AWS.\nSome event photos Add your screenshots/photos here\nIn short, the session outlines measurable GenAI steps across the SDLC: AI does the heavy lifting, while humans supervise and decide.\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: ‚ÄúAWS Cloud Day Vietnam 2025 ‚Äì AI Edition‚Äù Theme \u0026amp; Tagline: New Age Vietnam: From Vision to Value ‚Äî turning AI vision into measurable value.\nFocus: Generative AI with complementary Cloud capabilities (data, security, infrastructure, modern apps).\nEvent Objectives Drive Vision ‚Üí Value: tie GenAI to clear KPIs/ROI (speed, cost, quality). Present GenAI reference architectures (RAG, agentic workflows, data pipelines, guardrails). Define a practical adoption path: Idea ‚Üí PoC ‚Üí Pilot ‚Üí Scale with governance \u0026amp; data safety. Equip builders/engineers and IT leaders with AWS best practices (observability, cost control). Speakers AWS Vietnam leaders \u0026amp; specialists (AI/GenAI, Data, Security, AppMod). Customer speakers from Vietnam (transformation \u0026amp; GenAI use cases). Partners (deployment playbooks, cost optimization, operations). Key Highlights Central theme: Generative AI on AWS Amazon Bedrock / Amazon Q \u0026amp; Guardrails: rapid start, security, compliance. Data-first: retrieval/rerank/embeddings quality drives output quality. Cloud consistency: build on AWS foundations (identity, networking, storage, monitoring) for security \u0026amp; scale. Representative tracks GenAI Foundations \u0026amp; Architecture: RAG, tool-use/agents, evaluation \u0026amp; observability. Data, Security \u0026amp; Governance: PII protection, access control, moderation, cost/token tracking. App Modernization for AI: service decomposition, API-first, event-driven; making apps AI-ready. Builders Hands-on: demos/mini-labs for internal assistants and semantic search. Key messages Vision ‚Üí Value: avoid demo-only efforts; anchor to KPIs from day one. Data before models: optimize pipelines, indexing, caching, and latency. Safety \u0026amp; compliance: guardrails, auditability, continuous monitoring \u0026amp; evaluation. Key Takeaways Design Mindset Business-first: small, high-impact problems with clear owners \u0026amp; data. Ubiquitous language: align business‚Äìtech and lock KPIs/ROI early. Technical Architecture Prefer RAG and agentic patterns; separate orchestrator from tools. Observability \u0026amp; Eval: tracing, offline/online evaluation, feedback loops. Cost \u0026amp; Performance: fit-for-purpose models, batching/caching, prompt/context optimization, latency control. Modernization Strategy Roadmap PoC ‚Üí pilot ‚Üí limited rollout ‚Üí scale with exit criteria per stage. Build on AWS foundations for security, resilience, and scalability. Applying to Work Shortlist 1‚Äì2 GenAI use cases tied to clear KPIs; prepare data \u0026amp; access. Run a 6‚Äì8 week PoC with go/no-go gates (quality, latency, cost/interaction, adoption). Ops plan: monitoring, guardrails, human-in-the-loop, continuous improvement. Event Experience The event offered a clear path to move from vision to value: deploying GenAI into real workflows with security, cost awareness, and business alignment. Talks and demos showed how to standardize the data pipeline, implement RAG/agents, and measure impact to unlock pilot \u0026amp; scale.\nSome event photos Add your event photos here\nIn short, the ‚ÄúAI Edition‚Äù underscores that Generative AI creates real value when paired with the right data, architecture, governance, and KPIs on AWS.\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/5-workshop/5.1-workshop-overview/","title":"Overview","tags":[],"description":"","content":"Workshop Overview In this workshop, you will build a complete AI Chatbot using serverless architecture on AWS. The chatbot uses Claude Haiku 4.5 deployed entirely serverless with low costs and automatic scaling capabilities.\nModules Module 1: Setup Amazon Bedrock Module 2: Create Lambda Function Module 3: Configure API Gateway Module 4: Deploy Frontend to S3 Module 5: Testing \u0026amp; Debugging\nArchitecture Overview AWS Services \u0026amp; Their Roles in the Workshop 1. Amazon Bedrock ‚Äî AI Brain Process and generate responses from user messages. Uses Claude Haiku 4.5 (Messages API). Features: Inference Profiles Conversation history management Temperature control 2. AWS Lambda ‚Äî Backend Logic Role:\nReceive requests from API Gateway Validate input, format prompt Call Bedrock and return response Features:\nNode.js 24 runtime Environment variables (MODEL_ID, ALLOWED_ORIGIN) CloudWatch Logs Input validation + error handling Retry logic CORS handling 3. Amazon API Gateway ‚Äî API Endpoint Role:\nPublic REST API for browsers CORS management Route POST /chat to Lambda Throttling to prevent abuse Features:\nREST API or HTTP API CORS configuration Lambda Proxy Integration OPTIONS method for CORS preflight 4. Amazon S3 ‚Äî Frontend Hosting Static Website Hosting Host HTML/CSS/JS Allow users to access chatbot via browser 5. Amazon CloudWatch ‚Äî Monitoring \u0026amp; Debugging Collect logs from Lambda Debug request/response \u0026amp; errors Workshop Learning Objectives After the workshop, you will:\nUnderstand serverless architecture and how services interact Deploy a production-ready chatbot with Claude Haiku 4.5 Integrate Bedrock API with Lambda Configure API Gateway Host static website on S3 Debug using CloudWatch Logs Optimize serverless costs "},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AI-Driven Development Lifecycle: Reshaping Software Engineering\u0026rdquo; Event Information Date: Friday, October 3, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City\nOrganizer: AWS Idolize Team\nParticipants: Over 300 registered attendees\nFormat: In-person Event at Bitexco Tower\nEvent Agenda 14:00 - 14:15: Reception 14:15 - 15:30: Overview of AI Development Lifecycle and Amazon Q Developer Demo (by To√†n Hu·ª≥nh) 15:30 - 15:45: Break 15:45 - 16:30: Kiro Demonstration (by My Nguyen) Event Context The rise of generative AI marks a revolutionary shift in software development. Generative AI is reshaping how developers and organizations learn, plan, create, deploy, and securely manage applications.\nBy integrating AI into the software development lifecycle - from architecture to development, testing, deployment, and maintenance, developers can automate undifferentiated heavy-lifting tasks. This automation boosts productivity and enables developers to focus on more creative, high-value tasks.\nEvent Objectives Explore AI-Driven Development using Amazon Q Developer and Kiro Introduce AI-powered Software Development Lifecycle (AIDOC) methodology Demonstrate practical applications of AI in software development Showcase Amazon Q Developer capabilities Present Kiro tool for enhanced developer productivity Share best practices for integrating AI into development workflows Speakers To√†n Hu·ª≥nh ‚Äì Senior Solutions Architect, AWS M·ªπ Nguy·ªÖn ‚Äì Senior Prototyping Engineer, AWS Coordinators Di·ªÖm My ‚Äì AWS Team ƒê·∫°i Tr∆∞·ªùng ‚Äì AWS Team ƒêinh Nguy√™n ‚Äì AWS Team Key Highlights The Need for AI in Software Development Many developers are already using AI tools to increase productivity, but challenges remain:\nSimple tasks work well, but complex projects face issues Lack of methodology for integrating AI across the full development lifecycle Quality control concerns when AI generates large amounts of code Context management issues with token limits in large projects AI Development Lifecycle (AIDOC) Methodology AIDOC is not just a tool‚Äîit\u0026rsquo;s a comprehensive methodology that helps developers work effectively with AI throughout the software development process.\nCore Principles:\nHuman-AI Collaboration: AI assists, but humans validate, make decisions, and oversee Multi-step Problem Solving: Break down complex tasks instead of single-shot prompts Validation at Every Stage: Review AI outputs before proceeding to the next step Developer Ownership: Developers remain responsible for all code quality Three Levels of AI Integration AI Assistant (2023): Line-by-line code suggestions and auto-completion AI Assistant+ (2024): Solves larger tasks, provides solution options AI Agents (2025): Autonomous reasoning, planning, and multi-file generation Key Challenges When Using AI Single-shot limitations: Generic prompts produce generic results Token overflow: Context limits when working on large projects Existing codebase integration: Difficulty implementing features in established projects Quality assurance: Ensuring AI-generated code meets standards Decision making: Determining what AI should handle vs. human judgment Context management: Providing relevant information to AI Validation complexity: Reviewing and approving AI outputs AIDOC Workflow Instead of asking AI to complete entire features at once, AIDOC follows a structured approach:\nUnderstanding: AI helps analyze user stories and requirements Planning: AI generates implementation plans for developer review Design: Create logical design and identify affected layers Implementation: AI implements approved plans step-by-step Testing: Validate outputs at each stage Review: Human oversight ensures quality and correctness Amazon Q Developer Demonstration To√†n Hu·ª≥nh demonstrated how Amazon Q Developer integrates into the development workflow:\nIntelligent code completion with context awareness Multi-file code generation for complex features Architectural guidance for solution design Security scanning and best practice recommendations Natural language interaction for problem-solving Developer Responsibilities in AI-Powered Development Validation: Verify AI-generated code and designs Decision Making: Choose between AI-suggested options Oversight: Maintain control over the development process Quality Assurance: Ensure code meets standards Authorship: Take ownership of all delivered code Key Takeaways AI won\u0026rsquo;t replace developers, but developers who use AI effectively will outperform those who don\u0026rsquo;t Methodology matters: Having a structured approach (like AIDOC) is crucial for success Human oversight is essential: Developers must validate, decide, and oversee all AI outputs Productivity gains are real: Developers can increase output from 50 to 70+ user stories per month Multi-step approach works best: Break complex tasks into manageable steps Context is critical: Provide AI with relevant information for better results Quality remains human responsibility: Developers are the authors of their code Personal Insights This workshop highlighted how AI is transforming software development from a tool perspective to a methodology shift. The emphasis on human-AI collaboration rather than replacement was particularly important. The AIDOC framework provides a practical approach to integrating AI while maintaining quality and developer control.\nThe demonstrations of Amazon Q Developer and Kiro showed that AI tools have matured significantly, moving from simple code completion to complex multi-file generation and architectural guidance. However, the key message remains: developers must understand, validate, and own their code regardless of how it\u0026rsquo;s generated.\nUpcoming AWS Events Gen AI Game Day (Next month): Hands-on competition using Gen AI tools December Competition: Showcase Gen AI products (10-15 minute presentations) Applying to Work Adopt AIDOC methodology: Structure AI interactions in multi-step workflows Use Amazon Q Developer: Integrate into daily development for productivity boost Implement validation checkpoints: Review AI outputs at each development stage Break down complex tasks: Divide large features into manageable AI-assisted steps Maintain code ownership: Ensure understanding and quality of all AI-generated code Experiment with AI agents: Explore autonomous code generation for appropriate use cases Event Experience Attending the \u0026ldquo;AI Software Development Lifecycle\u0026rdquo; workshop was extremely valuable, providing comprehensive insights into modern AI-powered development practices. Key experiences included:\nLearning from Industry Experts AWS senior engineers shared real-world experiences using AI in enterprise projects Practical demonstrations showed both capabilities and limitations of current AI tools Understanding the evolution from simple AI assistants to autonomous AI agents Hands-on Technical Knowledge Witnessed live demonstrations of Amazon Q Developer solving complex coding challenges Learned the AIDOC methodology for structuring AI-assisted development Understood how to manage context and token limits in large projects Discovered techniques for breaking down user stories for AI implementation Mindset Transformation Realized AI is a collaborator, not a replacement for developers Understood the importance of validation and ownership in AI-generated code Learned to think in terms of multi-step problem solving rather than single prompts Recognized the value of methodology over tools alone Networking and Community Connected with 300+ developers interested in AI-powered development Exchanged ideas about AI integration challenges and solutions Learned about upcoming AWS community events and competitions Practical Tools and Techniques Amazon Q Developer for intelligent code completion and generation Kiro tool for enhanced productivity Multi-step prompting strategies for better AI responses Context management techniques for large codebases Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also reshaped my understanding of how AI can enhance the software development lifecycle. The emphasis on human oversight and validation ensures that AI serves as a powerful tool to amplify developer capabilities rather than replace them.\nConclusion The AI Software Development Lifecycle represents a fundamental shift in how we approach software development. By combining AI capabilities with human expertise through structured methodologies like AIDOC, developers can achieve significant productivity gains while maintaining quality and control. The key is understanding that AI is a powerful collaborator that requires proper guidance, validation, and oversight to deliver optimal results.\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"k6 Document: Cloud Cost Optimization Strategies and Tools for AWS, Azure, and GCP One of the biggest challenges in the cloud is cost optimization. If you don‚Äôt control costs, they can skyrocket. (And no one wants to be summoned by the finance team or the CFO to explain runaway cloud spending.)\nThat‚Äôs why it‚Äôs best to proactively manage cloud costs and optimize from day one. In this article, we‚Äôll look at practical ways to manage costs across the three major providers: Azure, AWS, and GCP.\nWhat happens when you don‚Äôt control cloud costs Failing to optimize early can lead to very real, painful problems:\nRunaway spending. Without limits or budgets, costs can climb rapidly. Budget overruns and missed KPIs. Projects overspend, casting doubt on ‚Äúcloud-first‚Äù strategies. Higher risk of service suspension. Especially for startups or capped usage, late payments or overages can trigger throttling or pauses. Overprovisioned and wasted resources. You pay for unused compute, storage, and networking that deliver no business value. Reduced flexibility. Overspending forces overly strict controls just to ‚Äúcourse-correct.‚Äù Slower time-to-market. Teams hesitate to launch resources because costs are unpredictable. Lost competitive edge. Poor cloud efficiency can make you pay 2‚Äì3√ó competitors for the same capability. Eroded leadership trust. Persistent overruns without comparable value make leaders question cloud as a strategy. Optimize before things get worse The most dangerous aspect of unoptimized cloud costs is compounding over time‚Äîlike interest. Small gaps become heavy burdens, shrinking your ability to innovate. The longer you wait, the more expensive and complex the cleanup becomes‚Äîclassic technical debt.\nKey mindset: treat cost optimization like tending a garden‚Äîregular care, pruning, and adjustments. Start with the biggest, easiest wins, then move into deeper optimizations.\nPlatform-agnostic cost strategies Each major cloud offers tools and methods to help you improve cost and performance. Below are general strategies, followed by provider-specific tips for AWS, Azure, and GCP.\n1) Right-size resources Inventory what you run and how it‚Äôs used. If something is underutilized, downsize or reconfigure it.\nAzure: use Azure Advisor to find underutilized VMs. AWS: use Compute Optimizer or Trusted Advisor for EC2, RDS, and Lambda right-sizing. GCP: use Recommender to tune machine types or autoscaler settings for Compute Engine. üëâ Kick off your AWS journey with Managing Compute Costs in AWS (Pluralsight).\n2) Clean up idle resources Cloud can turn into Hoarders fast. Teams create resources; people change roles; VMs, databases, and storage linger unowned. Nobody deletes them for fear of breaking something‚Äîclassic cloud sprawl.\nFix it:\nKeep good documentation. Audit regularly and remove what‚Äôs not needed. Automate cleanup with scripts or policy as code (e.g., Terraform + scheduled jobs). 3) Optimize storage costs Storage often drives surprise bills. Teams pick the wrong tiers or leave orphaned data in expensive classes.\nDo this:\nTrain teams on tiers and their purposes. Define clear allocation and ownership policies. Use archive tiers for cold data (backups, logs, compliance). Automation helpers:\nAzure: Blob Lifecycle Management ‚Üí shift to Cool/Archive. AWS: S3 Lifecycle Policies, Intelligent-Tiering ‚Üí move to Glacier/Deep Archive. GCP: Object Lifecycle Management ‚Üí Standard ‚Üí Nearline/Coldline. 4) Reduce network egress and fix traffic patterns Network fees are like toll roads‚Äîsmall individually, big in aggregate. Egress charges can silently drain budgets. Understand flows and architect to minimize unnecessary data movement.\nTactics:\nData locality planning: keep data near compute. E.g., app in us-east with DB in Europe = expensive egress. Compression \u0026amp; optimization: enable gzip, optimize images/video, dedupe backups. Traffic analysis: use AWS Cost Explorer, Azure Cost Management, GCP Cloud Billing to spot anomalies. Caching: CDN, app-level caches, DB query caching, API caching. Provider aids:\nAzure: ExpressRoute, Traffic Manager. AWS: VPC Endpoints, Global Accelerator, consolidate data regions. GCP: Private Google Access, co-locate services to cut egress. 5) Use AI-powered recommendations Each cloud ships ‚Äúcost whisperers‚Äù that suggest optimizations:\nAzure: Azure Advisor AWS: Compute Optimizer, Cost Anomaly Detection GCP: Recommender API, Active Assist These aren‚Äôt ‚Äúnice-to-haves‚Äù‚Äîthey‚Äôre your first line of defense against cost creep. Review and apply recommendations regularly.\n6) Continuous cost monitoring Cost tracking isn‚Äôt a one-time task‚Äîit‚Äôs continuous. Ongoing monitoring catches inefficiencies early and prevents surprise bills. Set up budgets, alerts, and forecasting, using native or third-party tools.\nNative tools:\nAzure: Cost Management, Azure Monitor AWS: CloudWatch, Cost Explorer, Budgets GCP: Billing Dashboard, Cloud Monitoring, BigQuery exports Provider tips:\nAzure: Use Azure Hybrid Benefit (for Windows Server licenses) + Cost Management analytics. AWS: Lean on Trusted Advisor + Cost Explorer for patterns and anomalies. GCP: Use sustained use discounts for long-running workloads and per-second billing for short jobs. Conclusion Hopefully this gives you practical strategies to start optimizing cloud costs today. You now have a strategy toolbox for AWS, Azure, and GCP. Pick your biggest cost driver and start there‚Äîyour budget will thank you.\nLevel up your team‚Äôs cloud skills with Pluralsight‚Äîhands-on labs, deep resources, and comprehensive cloud courses.\nAuthor\nSteve Buchanan ‚Äî Principal Program Manager at a global technology enterprise focused on improving cloud computing. Pluralsight author; author of eight technical books; Onalytica Who‚Äôs Who in Cloud? Top 50; former 10-time Microsoft MVP. Speaker at DevOps Days, Open Source North, Midwest Management Summit (MMS), Microsoft Ignite, BITCon, Experts Live Europe, OSCON, Inside Azure Management, keynote at Minnebar 18, and many user groups. Featured on numerous podcasts and in publications including the Star Tribune (5th-largest U.S. newspaper). He blogs at www.buchatech.com.\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Non-disruptive streaming with Olyzon and AWS Elemental MediaTailor preconditioned ads Author: Thomas Buatois ‚Äî June 24, 2025\nCategories: AWS Elemental MediaTailor, AWS Marketplace, Industries, Media \u0026amp; Entertainment, Media Services, Monetization, Partner solutions\nPermalink | Share\nStreaming video providers face the ongoing challenge of maximizing ad revenue while maintaining viewer engagement. The challenge lies in delivering seamless advertisements (ads) that integrate into content without causing buffering, quality degradation, or playback interruptions that drive viewers away.\nAWS Partner ‚Äì Olyzon has developed an innovative solution using preconditioned ads from AWS Elemental MediaTailor to achieve this balance.\nIntroducing preconditioned ads for enhanced ad insertion MediaTailor supports preconditioned ads, a feature that enables custom control over the ad transcoding process. This feature, also known as bring-your-own-ads through VAST responses, allows ad decision servers (ADS) to include HLS and DASH manifest URLs for multi-bitrate ad streams that have been pre-transcoded directly in the VAST XML creative file attribute.\nMediaTailor can now stitch these pre-transcoded ad creatives into the manifest without further transcoding. Previously, MediaTailor could only dynamically transcode ads to match the content stream at the time of insertion, causing potential delays in the ad delivery process.\nHow preconditioned ads transform the workflow In the traditional ad insertion workflow, MediaTailor must dynamically transcode ads to match the content stream, store them, and then stitch them into the live stream. This process introduces delays because MediaTailor must wait to receive ads from the ADS\u0026rsquo;s VAST responses before starting transcoding and stitching.\nPreconditioned ads help reduce the time required to insert ads into content by eliminating the transcoding step. These ads must be prepared to match the content stream and already transcoded before use with MediaTailor. The ADS VAST response must include direct links to the pre-transcoded HLS and DASH manifests. MediaTailor only needs to register the ad and stitch it into the content stream, reducing the time between receiving the VAST response and completing ad insertion.\nVAST response requirements for preconditioned ads Unlike typical VAST responses, where MediaTailor receives single creatives and transcodes them into multiple bitrates itself, the ADS must provide correctly formatted VAST responses with pre-transcoded assets.\nVAST Example:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;VAST xmlns:xsi=\u0026#34;[http://www.w3.org/2001/XMLSchema-instance](http://www.w3.org/2001/XMLSchema-instance)\u0026#34; version=\u0026#34;3.0\u0026#34;\u0026gt; \u0026lt;Ad id=\u0026#34;ad1\u0026#34;\u0026gt; \u0026lt;InLine\u0026gt; \u0026lt;AdSystem\u0026gt;ExampleAdSystem\u0026lt;/AdSystem\u0026gt; \u0026lt;AdTitle\u0026gt;ad1\u0026lt;/AdTitle\u0026gt; \u0026lt;Impression\u0026gt;\u0026lt;![CDATA[[https://example-impression.amazonaws.com](https://example-impression.amazonaws.com)]]\u0026gt;\u0026lt;/Impression\u0026gt; \u0026lt;AdServingId\u0026gt;de8e0d33-9c72-4d77-bb3a-f7e566ffc605\u0026lt;/AdServingId\u0026gt; \u0026lt;Creatives\u0026gt; \u0026lt;Creative id=\u0026#34;creativeId1\u0026#34; sequence=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;Linear skipoffset=\u0026#34;00:00:05\u0026#34;\u0026gt; \u0026lt;Duration\u0026gt;00:00:30\u0026lt;/Duration\u0026gt; \u0026lt;MediaFiles\u0026gt; \u0026lt;MediaFile delivery=\u0026#34;progressive\u0026#34; width=\u0026#34;1280\u0026#34; height=\u0026#34;720\u0026#34; type=\u0026#34;video/mp4\u0026#34; bitrate=\u0026#34;533\u0026#34; scalable=\u0026#34;true\u0026#34; maintainAspectRatio=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;![CDATA[[https://example-ad-origin.amazonaws.com/ad1/ad1.mp4](https://example-ad-origin.amazonaws.com/ad1/ad1.mp4)]]\u0026gt;\u0026lt;/MediaFile\u0026gt; \u0026lt;MediaFile delivery=\u0026#34;streaming\u0026#34; width=\u0026#34;1280\u0026#34; height=\u0026#34;720\u0026#34; type=\u0026#34;application/dash+xml\u0026#34; bitrate=\u0026#34;533\u0026#34; scalable=\u0026#34;true\u0026#34; maintainAspectRatio=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;![CDATA[[https://example-ad-origin.amazonaws.com/ad1/index.mpd](https://example-ad-origin.amazonaws.com/ad1/index.mpd)]]\u0026gt;\u0026lt;/MediaFile\u0026gt; \u0026lt;MediaFile delivery=\u0026#34;streaming\u0026#34; width=\u0026#34;640\u0026#34; height=\u0026#34;360\u0026#34; type=\u0026#34;application/x-mpegURL\u0026#34; bitrate=\u0026#34;262\u0026#34; scalable=\u0026#34;true\u0026#34; maintainAspectRatio=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;![CDATA[[https://example-ad-origin.amazonaws.com/ad1/index_low.m3u8](https://example-ad-origin.amazonaws.com/ad1/index_low.m3u8)]]\u0026gt;\u0026lt;/MediaFile\u0026gt; \u0026lt;MediaFile delivery=\u0026#34;streaming\u0026#34; width=\u0026#34;2560\u0026#34; height=\u0026#34;1440\u0026#34; type=\u0026#34;application/x-mpegURL\u0026#34; bitrate=\u0026#34;1066\u0026#34; scalable=\u0026#34;true\u0026#34; maintainAspectRatio=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;![CDATA[[https://example-ad-origin.amazonaws.com/ad1/index_high.m3u8](https://example-ad-origin.amazonaws.com/ad1/index_high.m3u8)]]\u0026gt;\u0026lt;/MediaFile\u0026gt; \u0026lt;/MediaFiles\u0026gt; \u0026lt;/Linear\u0026gt; \u0026lt;/Creative\u0026gt; \u0026lt;/Creatives\u0026gt; \u0026lt;/InLine\u0026gt; \u0026lt;/Ad\u0026gt; \u0026lt;/VAST\u0026gt; "},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Register Now: Updated Exam and New Name for Cloud Operations Certification Author: Lauren Ebbin ‚Äî July 15, 2025 Categories: Announcements, AWS Training and Certification, DevOps Permalink | Share\nEdited on September 9, 2025, to announce that registration is now open and new exam preparation resources are available on AWS Skill Builder.\nRegistration is now open for the AWS Certified CloudOps Engineer ‚Äì Associate exam (previously known as the AWS Certified SysOps Administrator ‚Äì Associate) to align with the latest skills and knowledge in monitoring and maintaining Amazon Web Services (AWS) workloads. The last day to take the AWS Certified SysOps Administrator ‚Äì Associate (SOA-C02) exam will be September 29, 2025.\nFrom SysOps to CloudOps We are renaming this certification to AWS Certified CloudOps Engineer ‚Äì Associate to reflect the evolving nature of cloud operations and the change in industry terminology. This change aims to enhance the relevance and credibility of individuals who earn this certification, highlighting their knowledge and skills in deploying, operating, and maintaining workloads on AWS.\nFrom SOA-C02 to SOA-C03 We have updated the exam with the help of subject matter experts (SMEs) and will release a new exam guide on September 9, 2025. A key difference between AWS Certified SysOps Administrator ‚Äì Associate (SOA-C02) and AWS Certified CloudOps Engineer ‚Äì Associate (SOA-C03) is that containers are now in scope for SOA-C03.\nWe have included more modern services and features, increased emphasis on multi-account, multi-Region architectures, and more on automation and infrastructure as code. No task statements were removed between exam versions, but the new content outline is more detailed with some reorganization of the task statements.\nCertification maintenance and badges The name change to AWS Certified CloudOps Engineer ‚Äì Associate will only apply to those who pass the SOA-C03 exam. The name will not be retroactively changed for current holders of the AWS Certified SysOps Administrator ‚Äì Associate certification. Here is what you need to know about certification validity, recertification, and maintaining certified status:\nIndividuals who earned the AWS Certified SysOps Administrator ‚Äì Associate certification are not required to earn the AWS Certified CloudOps Engineer ‚Äì Associate certification. They can continue to display their certification badge, which will remain active until its expiration date. Individuals who earned the AWS Certified SysOps Administrator ‚Äì Associate certification and subsequently earn the AWS Certified CloudOps Engineer ‚Äì Associate certification will have a separate badge for each. Their AWS Certified SysOps Administrator ‚Äì Associate certification will remain active until its expiration date. In the future, these individuals can choose to: Continue to recertify the AWS Certified CloudOps Engineer ‚Äì Associate certification by passing the latest version of the exam. Recertify both certifications by upgrading to the AWS Certified DevOps Engineer ‚Äì Professional certification before the certifications‚Äô expiration date. Individuals with active AWS Certified DevOps Engineer ‚Äì Professional and AWS Certified SysOps Administrator ‚Äì Associate certifications can continue to recertify both by passing the latest version of the AWS Certified DevOps Engineer ‚Äì Professional exam. Get ready with AWS Skill Builder The Exam Prep Plan: AWS Certified CloudOps Engineer ‚Äì Associate (SOA-C03) is now available on AWS Skill Builder.\nThe Exam Prep Plan includes:\npractice assessments with exam-style questions hands-on practice through AWS SimuLearn review courses of each exam domain The Plan also references role-based training to refresh your AWS knowledge and skills.\nResources AWS Training and Certification Get Trained Get Certified Develop Your Team AWS Partner Training AWS Educate "},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Make new friends and find your group. Learn how to self-study from scratch and complete the challenge to get $100 Free tier. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Find and get to know your group mates. - Read and learn the rules and regulations on how to fill out the registration form at the office. - Read and learn the rules and regulations on how to fill out the registration form at the office. - Learn how to get to the company and get a visitor card, and park your vehicle at the internship company. 08/09/2025 08/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Read and learn more about AWS to know what the upcoming program will do like workshops or projects\u0026hellip;.. - Find the link to write a worklog and write a worklog. 09/09/2025 09/09/2025 https://workshop-sample.fcjuni.com/1-worklog/ 4 - Learn and practice how to create an AWS Free Tier account. - Learn about root user and IMA account. - Practice: + Create AWS account. + Learn how to create and delete VCP. + Learn how to create and delete EC2. 10/09/2025 10/09/2025 https://aws.amazon.com/profile 5 - Basic understanding and challenges that the account offers to receive an additional $100. - Read and learn the first 2 challenges to receive $40. - Practice: + Take the challenge and complete challenge 2. + Take the challenge and complete challenge 3. + Learn how to draw clouds on draw.io. 11/09/2025 11/09/2025 https://us-east-1.console.aws.amazon.com/billing/home#/credits 6 - Learn and how to do 3 challenges to get the remaining $60 on your AWS account. - Practice: + Complete Challenge 3. + Complete Challenge 4. + Complete Challenge 5. 12/09/2025 12/09/2025 https://us-east-1.console.aws.amazon.com/billing/home#/credits Week 1 Achievements: Register to go to the office with the team to learn and know how to use aws.\nSuccessfully created and configured an AWS Free Tier account.\nGet familiar with aws root and aws IMA accounts, learn how to search, access and use services, complete tasks to receive rewards from that service.\nInstalled and configured AWS and know how to use the services on the computer, including:\nEC2. VCP. Learn how to create and delete services to avoid being charged. Learn and draw from the sample from draw.io about cloud drawings. \u0026hellip; Used AWS CLI to perform basic operations such as:\nKnow how to manage accounts. Check daily balance to see if there are any services running in the background that cost money. View EC2 service. Learn how to create and delete services to avoid being charged. Check information about running services. \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Basic understanding of AWS Cloud9, S3, RDS (Read and learn). Complete Lab S3 (Tuesday). Participate in Cloud Day Vietnam event and learn about AI trends (Wednesday). Complete Lab RDS (Fixed and completed on Friday). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Read and explore AWS Cloud9 (note: access is currently closed in this application). - Read and learn Amazon S3 to practice and do labs. - Continue reading and learning Amazon RDS to practice and do labs. 14/09/2025 16/09/2025 https://000057.awsstudygroup.com/vi/1-introduce/ 3 - Complete the lab and understand the purpose of the S3 Lab: - Practice: + Store objects. + Use data management features. + Apply management and security. + \u0026hellip; 17/09/2025 17/09/2025 https://us-east-1.console.aws.amazon.com/s3/bucket/create?region=us-east-1\u0026bucketType=general 4 - Join the Cloud Day Vietnam event and learn a lot from the event (also the first event at FCJ). - Learn current trends and how Vietnamese companies and business leaders are using AI to automate, increase revenue, and optimize processes. 18/09/2025 18/09/2025 https://vmxwvcrs.r.us-east-1.awstrack.me/L0/https:%2F%2Femail.awscloud.com%2FMTEyLVRaTS03NjYAAAGc9dyF5F1pH9rPDZl68ocGtzzZO0RNKl9nlzWVoHl19AV5GA2hLENGpcAopvy9k4xd-U4R2vg=/1/0100019956e3db6c-8f5a7a13-b84e-4b95-b82f-414391a6af17-000000/aOaA73q6gzphysKAm7ScQrPg4Fo=444 5 - Worked on the RDS lab but faced errors all day and couldn‚Äôt fix them. - Read the web instructions and prepared to practice to complete the lab. - Practice: + Follow the instructions. + Create tables as instructed. + \u0026hellip;; 19/09/2025 19/09/2025 https://us-east-1.console.aws.amazon.com/rds/home?region=us-east-1# 6 - Practice: + Fixed the errors + Recreated tables and continued completing RDS + Completed the RDS lab 20/09/2025 20/09/2025 https://us-east-1.console.aws.amazon.com/rds/home?region=us-east-1# Week 2 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Conquer Amazon EC2: Understand how to initialize virtual servers, choose configuration (Instance types) and manage hard drives (EBS). Master Amazon VPC: Understand how to design virtual networks, subnet and control access flows. Connect infrastructure: Practice connecting Web Server (EC2) to Database (RDS) created in week 2. Tasks to be carried out this week: Day Task Start Date End Date Resource 2 - Compute \u0026amp; Storage Theory (Block): - Read about Amazon EC2: Instance types (T, M, C\u0026hellip;), Lifecycle. - Learn Amazon EBS: Volume types (GP3, IO2\u0026hellip;), how to mount volumes to EC2. - Learn about AMI (Amazon Machine Image). 23/09/2025 23/09/2025 https://explore.skillbuilder.aws/learn 3 - Complete lab and understand EC2 Lab goals: - Practice: + Create Key Pair and Security Group + Launch an EC2 Instance + SSH into server and install Web Server + ‚Ä¶ 24/09/2025 24/09/2025 https://us-east-1.console.aws.amazon.com/ec2/home?region=us-east-1 4 - Learn Networking Theory (VPC): - Read about VPC architecture: CIDR block, Subnet (Public vs Private). - Learn Internet Gateway (IGW) and Route Table. - Distinguish Security Group (Stateful) vs NACL (Stateless). 25/09/2025 25/09/2025 https://docs.aws.amazon.com/vpc/ 5 - Do VPC lab to design system network. - Read instructions and prepare steps to create Custom VPC. - Practice: + Create new VPC and divide Subnets + Attach Internet Gateway and configure Route Table + ‚Ä¶ 26/09/2025 26/09/2025 https://us-east-1.console.aws.amazon.com/vpc/home?region=us-east-1 6 - Integration Practice: + Connect Web Server (EC2) with Database (RDS) created last week + Configure Security Group to allow port 3306 + Complete basic 2-Tier architecture 27/09/2025 27/09/2025 https://us-east-1.console.aws.amazon.com/ec2/v2/home Week 3 Outcomes: Mastered foundational knowledge of Compute (EC2) and Networking (VPC). Completed labs on creating virtual servers and designing VPC networks. Learned how to use Security Groups to secure and control access between resources. Completed integration challenge: Successfully connected Web Server (EC2) and Database (RDS). Successfully built a basic 2-Tier architecture on AWS. \u0026hellip; "},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Complete the design of the homepage (Homepage UI). Creatively design the auxiliary pages: Policy \u0026amp; Returns, Shipping, FAQ. Plan the design for the product catalog (Product Catalog). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Continue designing in Figma and learning additional topics. - Clean the web framework (remove errors, do a clean build). 29/09/2025 03/10/2025 https://www.figma.com/design/U72937c9sp4cD0O5Fcwlw0/AWS-2025?node-id=81-3129\u0026t=mlq1HOld0udU3Bk1-0 3 - Register and do the internship at the office. Learn and understand S3 to support the Back-End with data. - Learn and design Figma for the next day‚Äôs web design. 29/09/2025 03/10/2025 https://www.figma.com/design/U72937c9sp4cD0O5Fcwlw0/AWS-2025?node-id=81-3129\u0026t=mlq1HOld0udU3Bk1-0 4 - First step: create the Home page and optimize the Header and Footer. - Creatively design Policy \u0026amp; Returns, Shipping, etc., to build a Help/FAQ section for users. 29/09/2025 03/10/2025 https://www.figma.com/design/U72937c9sp4cD0O5Fcwlw0/AWS-2025?node-id=81-3129\u0026t=mlq1HOld0udU3Bk1-0 5 - Plan the web design based on the Figma and complete the HOMEPAGE UI. - Collect and save product images to add products to the website; discuss and plan pages/accounts for admin, staff, etc. 29/09/2025 03/10/2025 https://www.figma.com/design/U72937c9sp4cD0O5Fcwlw0/AWS-2025?node-id=81-3129\u0026t=mlq1HOld0udU3Bk1-0 6 - Continue the design and create site-wide footer items for the product catalog to bring products into the UI. - Plan weekend work and set completion goals. 29/09/2025 03/10/2025 https://www.figma.com/design/U72937c9sp4cD0O5Fcwlw0/AWS-2025?node-id=81-3129\u0026t=mlq1HOld0udU3Bk1-0 Week 4 Achievements: Continued developing design skills: drew more in Figma; cleaned and stabilized the web framework. Joined the internship; learned S3 to support Back-End data usage. Designed Figma sections (Policy \u0026amp; Returns, Shipping, etc.) to form a Help/FAQ area. Implemented the HOMEPAGE UI and planned admin/staff account pages. Finalized footer items and brought products into the UI; planned weekend work with clear goals. \u0026hellip; "},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Develop the Administration Subsystem (Admin \u0026amp; Staff Dashboard): Complete the Figma design and deploy the Front-End for the login pages, Admin and Staff management pages. Unify the Architecture with the Back-End: Unify the AWS services to be used and the communication process between FE and BE (API). Solve technical problems (Troubleshooting): Act as technical support, fix errors (blockers) that teammates encounter to ensure the overall progress. Prepare sample data (Mocking): Create sample product data on the interface while waiting for the Back-End to complete the API. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Completed the weekend plan: added the product section interface in the footer and designed Figma screens for admin and staff. - Same day: office registration was unsuccessful, so continued Front-End work. 06/10/2025 10/10/2025 https://github.com/khanhtm45/AWS/tree/main/frontend https://github.com/khanhtm45/AWS/tree/main/backend 3 - Worked with teammates on login and page designs for admin and staff; fixed blockers teammates couldn‚Äôt resolve. - Completed login testing for admin/staff and designed their interfaces. 06/10/2025 10/10/2025 https://github.com/khanhtm45/AWS/tree/main/frontend https://github.com/khanhtm45/AWS/tree/main/backend 4 - Met with Back-End (BE) to align on appropriate AWS services for the project. - Handed over assigned tasks; the team reviewed for gaps/overlaps and provided feedback to proceed with next interfaces. 06/10/2025 10/10/2025 https://github.com/khanhtm45/AWS/tree/main/frontend https://github.com/khanhtm45/AWS/tree/main/backend 5 - Added product images and demo product details while waiting for BE to finish API \u0026amp; Database. - Completed login tests and finalized admin/staff interfaces. 06/10/2025 10/10/2025 https://github.com/khanhtm45/AWS/tree/main/frontend https://github.com/khanhtm45/AWS/tree/main/backend 6 - Followed up with teammates to complete leader-assigned tasks. - Translated blog posts as a side task requested by advisors; delivered in .docx, minimizing code. 06/10/2025 10/10/2025 https://github.com/khanhtm45/AWS/tree/main/frontend https://github.com/khanhtm45/AWS/tree/main/backend Week 5 Achievements: Completed footer product section and Figma designs for admin/staff; continued Front-End when office registration didn‚Äôt go through. Built and tested login flows; fixed blockers for teammates; finalized admin/staff interfaces. Synced with Back-End on AWS service choices; performed team handover/review for next UI steps. Prepared data and visuals (product images, demo details) while waiting for API/DB completion. Supported the team: followed up on tasks; translated advisor-requested blog posts into .docx with minimal code. \u0026hellip; "},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Prepare lessons and study for exams. Draw the structure to send to the Mentor for check. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Plan to prepare for the exam, prepare questions to study and review knowledge. - Read carefully the review content that the mentors sent and stick to the exam questions. 13/10/2025 15/10/2025 https://www.notion.so/Nguy-n-Duy-Hi-u-26b17fef5a7781bf8f71e6846c27ad86 3 - Continue studying for the exam and take a day to review the practical knowledge and labs. 13/10/2025 15/10/2025 https://www.notion.so/Nguy-n-Duy-Hi-u-26b17fef5a7781bf8f71e6846c27ad86 4 - Find information and review exercises that you can use to review for your upcoming midterms. - Looking for exam questions to follow and continuing the unfinished project. 13/10/2025 15/10/2025 https://www.notion.so/Nguy-n-Duy-Hi-u-26b17fef5a7781bf8f71e6846c27ad86 5 - Meet and support team members on project architecture drawing to apply aws technical design to the project. - Edit the drawing structure, research and the whole team works together to perfect the structure so that it can be submitted to the mentors to check if it is okay and can be applied to the project. 16/10/2025 18/10/2025 https://skillbuilder.aws/ 6 - But the project failed due to drawing errors. And using icons that were sleeping and other reasons that were not suitable for project management costs, so the brothers suggested that it should be removed and the aws application structure should be redrawn. - Meet and re-plan to prepare for a more complete redraw of the structure. 16/10/2025 18/10/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Plan, prepare questions, and stick to the review content from mentors to prepare for the exam.\nContinue studying for the exam and spend a day reviewing practical knowledge and labs.\nFind materials and exercises for the midterm review and continue the unfinished project.\nMeet with the team, draw, and finalize the project\u0026rsquo;s AWS architecture structure for mentor approval.\nThe project architecture failed due to errors and costs; the team met to plan a redraw.\n\u0026hellip;\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Draw, design and plan your studies. Review for midterm exams. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Continue drawing and planning to prepare for the AI ‚Äã‚Äãchat box design consulting for the homepage. - Learn about Powered by AWS Bedrock AI and how to prepare it for your project on the homepage. 20/10/2025 20/10/2025 https://aws.amazon.com/vi/bedrock/?trk=58fd0fb1-df1c-4e26-b990-06d09d890997\u0026sc_channel=ps\u0026ef_id=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB:G:s\u0026s_kwcid=AL!4422!3!692062112144!p!!g!!bedrock!21054970946!157173566857\u0026gad_campaignid=21054970946\u0026gbraid=0AAAAADjHtp8Si2C_9mrOH-lEBa26rs-CT\u0026gclid=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB 3 - Decide to use a chat infrastructure powered by AWS Bedrock AI. - Write code and debug to ensure the chat box works smoothly. 21/10/2025 23/10/2025 https://aws.amazon.com/vi/bedrock/?trk=58fd0fb1-df1c-4e26-b990-06d09d890997\u0026sc_channel=ps\u0026ef_id=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB:G:s\u0026s_kwcid=AL!4422!3!692062112144!p!!g!!bedrock!21054970946!157173566857\u0026gad_campaignid=21054970946\u0026gbraid=0AAAAADjHtp8Si2C_9mrOH-lEBa26rs-CT\u0026gclid=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB 4 - After coding, with the support of AI, the chat box part can be basically completed. - Complete the interface design and discuss with BE so we can continue working on the inside of the chat box. 21/10/2025 23/10/2025 https://aws.amazon.com/vi/bedrock/?trk=58fd0fb1-df1c-4e26-b990-06d09d890997\u0026sc_channel=ps\u0026ef_id=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB:G:s\u0026s_kwcid=AL!4422!3!692062112144!p!!g!!bedrock!21054970946!157173566857\u0026gad_campaignid=21054970946\u0026gbraid=0AAAAADjHtp8Si2C_9mrOH-lEBa26rs-CT\u0026gclid=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB 5 -BE is assigning and waiting for assigned tasks. - Finalize the initial structure of the interface and team members are testing the features for bugs. 21/10/2025 23/10/2025 https://aws.amazon.com/vi/bedrock/?trk=58fd0fb1-df1c-4e26-b990-06d09d890997\u0026sc_channel=ps\u0026ef_id=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB:G:s\u0026s_kwcid=AL!4422!3!692062112144!p!!g!!bedrock!21054970946!157173566857\u0026gad_campaignid=21054970946\u0026gbraid=0AAAAADjHtp8Si2C_9mrOH-lEBa26rs-CT\u0026gclid=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB 6 - Study for midterms, do labs again so you can apply the definitions, learn theory that can be given in midterms. - Finalize the initial structure of the interface and team members are testing the features for bugs. 21/10/2025 23/10/2025 https://skillbuilder.aws/ Week 7 Achievements: Sketch and plan the AI chat box design consultation for the homepage.\nResearch ‚ÄúPowered by AWS Bedrock AI‚Äù and prepare integration for the homepage project.\nDecide to adopt a chat infrastructure powered by AWS Bedrock AI.\nImplement and debug the chat box to ensure smooth operation.\nBasically complete the chat box with AI support; finalize the UI and align with BE for internal logic.\nBE assigns tasks; finalize the initial UI structure, team tests features for bugs; study for midterms and redo labs.\n\u0026hellip;\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Study your lessons and theory for the midterm exam. Midterm exam. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Go to the office to study. - RStudy with mentors to prepare for midterm exams. 27/10/2025 31/10/2025 https://skillbuilder.aws/ 3 - Review your lessons with quizzes on quizlet. - Thorough study of solution architect theory. 27/10/2025 31/10/2025 https://quizlet.com/340744042/aws-flash-cards/?funnelUUID=62f88ee4-b28f-4699-b909-a959d8bd4efd 4 - View sample and code of SAA exam. - Use AI chat to create self-reading questions and choose test answers like an exam. 27/10/2025 31/10/2025 https://ezse.net/aws/certified-cloud-practitioner/cau-hoi-co-ban.html 5 - Learn the theory and application of aws for solutions. - Review the theory to see which aws applications are suitable for cost saving, dangerous conditions,\u0026hellip;.. 27/10/2025 31/10/2025 https://skillbuilder.aws/ 6 - Midterm exam. 31/10/2025 31/10/2025 Week 8 Achievements: Come to the office to study and review with a mentor to prepare for the midterm exam.\nReview with Quizlet question sets.\nLearn in-depth Solution Architect theory.\nView sample/code snippets of SAA; use AI to generate self-practice questions and choose answers like the real exam.\nLearn AWS theory \u0026amp; application for solutions; review appropriate options to save costs and handle risky situations.\nMidterm exam (October 31, 2025).\n\u0026hellip;\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Setup a parallel running environment (Frontend - Backend - Docker) to develop the project. Complete the integration of Product Management and Category APIs. Solve the challenge of displaying images from AWS S3. Tasks to be carried out this week: Day Task Start Date Completion Date Resource 2 - Integrate Backend API and test with Swagger before implementing API into Frontend. - Check Login functions to configure and add admin permissions. 03/11/2025 09/11/2025 http://localhost:8080/swagger-ui/index.html?gidzl=Un-z6C7ELdHx1huVmjDLSde1l1xRqL1wPbRiGjxKLtfc0EyGqOG4AMm1kKJItmKiFrplGZUFvjbSnyrKSW\u0026continue=#/product-variant-controller/listProductVariants 3 - Design Admin UI for Categories and Product Management. - Integrate Product Management and Category APIs. - Troubleshoot Backend issues regarding installation and synchronization with Docker. 03/11/2025 09/11/2025 http://localhost:8080/swagger-ui/index.html?gidzl=Un-z6C7ELdHx1huVmjDLSde1l1xRqL1wPbRiGjxKLtfc0EyGqOG4AMm1kKJItmKiFrplGZUFvjbSnyrKSW\u0026continue=#/product-variant-controller/listProductVariants 4 - Setup environment and link Backend Git. - Run the system in parallel between Backend, Docker, and Frontend. - Add new APIs and check system functionality. 03/11/2025 09/11/2025 http://localhost:8080/swagger-ui/index.html?gidzl=Un-z6C7ELdHx1huVmjDLSde1l1xRqL1wPbRiGjxKLtfc0EyGqOG4AMm1kKJItmKiFrplGZUFvjbSnyrKSW\u0026continue=#/product-variant-controller/listProductVariants 5 - Complete API integration for both sides. - AWS S3 Challenge: Handle logic to fetch images from S3 and display them on the Web. - Learn and research how to retrieve S3 Keys for display. - Find a way to add, edit, and delete displayed images via product categories using API. 03/11/2025 09/11/2025 http://localhost:8080/swagger-ui/index.html?gidzl=Un-z6C7ELdHx1huVmjDLSde1l1xRqL1wPbRiGjxKLtfc0EyGqOG4AMm1kKJItmKiFrplGZUFvjbSnyrKSW\u0026continue=#/product-variant-controller/listProductVariants 6 - Finalize Features: + Add, edit, delete products (CRUD) + Display the complete product list in the project management section. 03/11/2025 09/11/2025 http://localhost:8080/swagger-ui/index.html?gidzl=Un-z6C7ELdHx1huVmjDLSde1l1xRqL1wPbRiGjxKLtfc0EyGqOG4AMm1kKJItmKiFrplGZUFvjbSnyrKSW\u0026continue=#/product-variant-controller/listProductVariants Week 9 Achievements: Synchronized Development Environment Setup:\nSuccessfully installed and configured a parallel running environment between Backend, Docker, and Frontend. Completed Backend Git linking, ensuring source code synchronization. API Integration and Authorization:\nUnderstood and proficiently used Swagger to test APIs before integrating into Frontend. Successfully handled Login flow and Admin permission assignment. Product Management Module Completion:\nCompleted UI design for Product Management and Categories pages. Successfully integrated APIs for full CRUD functions (Create, Read, Update, Delete) for products. Solved Image Storage Challenge with AWS S3:\nGrasped the object storage mechanism on S3. Resolved the logic issue regarding retrieving S3 Keys from Backend and displaying product images on the Frontend interface. \u0026hellip; "},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Understanding AWS Cloud9, S3, RDS basics and completing Labs\nWeek 3: Mastering Amazon EC2, VPC and connecting Web Server infrastructure with Database\nWeek 4: Completing homepage design and supporting pages on Figma\nWeek 5: Developing Admin Dashboard and consolidating architecture with Backend\nWeek 6: Reviewing and preparing for exams, drawing project structure\nWeek 7: Designing and integrating AI chatbox using AWS Bedrock\nWeek 8: Studying theory and taking midterm exam\nWeek 9: Setting up environment and completing API integration for product management\nWeek 10: Integrating APIs into frontend and practicing AWS Workshop\nWeek 11: Completing basic project and preparing for submission\nWeek 12: Reviewing system, preparing demo and presentation materials\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/2-proposal/","title":"Proposal","tags":[],"description":"","content":"AWS First Cloud AI Journey ‚Äì Leaf E-commerce Project Plan 1. BACKGROUND and Motivation 1.1 Executive Summary Leaf is an e-commerce platform specializing in fashion products for men and women, including fashion accessories such as jewelry, shoes, hats, and leather straps. The website integrates AWS services to optimize costs and enhance the user experience.\n1.2 Project Success Criteria Fully functional e-commerce platform with AWS integration. Optimized cost and performance using serverless architecture. Fast page load with CDN (CloudFront) and S3 hosting. Seamless notifications using SNS/SES. AI-powered translation and assistance (optional) via Amazon Translate \u0026amp; Bedrock. 1.3 Assumptions Customers have basic knowledge of cloud services and AWS accounts. Serverless approach is acceptable; no dedicated servers required. Traffic is moderate (~few thousand users/month) and costs are expected to be low. Required services (S3, Lambda, DynamoDB, etc.) are accessible in selected AWS regions. Images and static assets will be stored in S3/CDN for performance. 2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 Technical Architecture Diagram The platform applies an AWS Serverless architecture to manage data.\nComponents and Roles in the AWS Architecture A. User Interface Layer Service Role Detailed Description AWS Amplify Website deployment Hosts static websites (React, Vue, Next.js) and automatically builds/deploys when code is pushed to GitHub. Amazon CloudFront (CDN) Improve page loading speed Caches static content close to users to reduce latency. Amazon S3 Store static files \u0026amp; product images Acts as a content repository for images, banners, CSS/JS files. B. Application Logic Layer Service Role Detailed Description Amazon API Gateway API gateway Receives requests from frontend and forwards them to Lambda functions for processing. AWS Lambda Server-side logic Handles orders, payments, authentication, email without dedicated servers. Amazon DynamoDB NoSQL database Stores products, accounts, orders, shopping carts. AWS Secrets Manager Secure sensitive data Stores API keys, payment tokens, database passwords. C. User Management \u0026amp; Security Layer Service Role Detailed Description AWS WAF Web protection Protects against SQL Injection, XSS, DDoS. Amazon Route 53 DNS \u0026amp; domain Manages domain names. D. Notification \u0026amp; Communication Layer Service Role Detailed Description Amazon SNS System notifications Sends notifications to admins or users. Amazon SES Transactional emails Sends order confirmations, promotions, password reset emails. E. AI \u0026amp; Machine Learning Layer Service Role Detailed Description Amazon Translate Content translation Translates product descriptions to other languages. Amazon Bedrock AI content generation Creates chatbots for shopping assistance. F. Monitoring \u0026amp; Management Layer Service Role Detailed Description Amazon CloudWatch System monitoring Monitors logs, performance, alerts for errors or cost spikes. AWS CloudTrail Administrative logging Tracks configuration changes for auditing purposes. 2.2 Technical Plan Collect system requirements and features. Estimate cost and check feasibility. Design UI prototypes using Figma. Build database schema and backend APIs. Develop frontend interface. Integrate AWS services (S3, Lambda, DynamoDB, etc.). Test and deploy system using serverless architecture. 2.3 Project Plan Agile Scrum framework, 8 √ó 2-week sprints. Sprint Reviews and Retrospectives conducted with stakeholders. Knowledge transfer sessions scheduled at end of each sprint. 2.4 Security Considerations Enable MFA on account access. Configure AWS CloudTrail \u0026amp; Config for monitoring. Apply WAF to block malicious requests. Encrypt sensitive data using Secrets Manager \u0026amp; AWS KMS. 3. Activities AND Deliverables 3.1 Activities and Deliverables Project Phase Timeline Activities Deliverables/Milestones Total man-day Assessment Week 1-2 Collect requirements, estimate costs Requirement document X man-day Setup Base Infrastructure Week 3-4 Provision S3, Amplify, CloudFront Working base environment X man-day Setup Components Week 5-6 API Gateway, Lambda, DynamoDB Backend \u0026amp; auth ready X man-day Testing \u0026amp; Go-live Week 7 Full integration testing Live system deployed X man-day Handover Week 8 Knowledge transfer \u0026amp; documentation Final deliverables accepted X man-day 3.2 Out of Scope Non-AWS hosting. Legacy system migrations. Custom AI/ML development beyond Translate/Bedrock. 3.3 Path to Production POC built for main use-cases. Production setup requires tuning for operational excellence. Error handling, logging, and testing fully implemented. 4. EXPECTED AWS COST BREAKDOWN BY SERVICES Service Group Total Cost (USD/month) Storage \u0026amp; Data 3.55 Backend \u0026amp; Processing 0.75 UI \u0026amp; Security 8.20 Email \u0026amp; Notifications 0.20 AI \u0026amp; ML (Optional) 0.25 Monitoring \u0026amp; Logs 1.50 Total (Actual) ‚âà 14.45 USD / month View AWS Pricing Calculator\n5. TEAM Name Student ID Nguy·ªÖn Tu·∫•n Ki·ªát SE182120 Nguy·ªÖn Thanh S∆°n SE183379 Tr∆∞∆°ng Minh Kh√°nh SE182131 Nguy·ªÖn VƒÉn Th√†nh SE193632 L√™ H·ªì Gia B·∫£o SE184518 "},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"IAM Permissions Create an IAM role with Permissions policies: AmazonBedrockFullAccess and CloudWatchLogsFullAccess\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Attach APIs piece by piece to the frontend and use F12 (DevTools) to check, remove, and minimize minor bugs. - Hold a meeting to choose a time for an offline session. 10/11/2025 16/11/2025 http://localhost:3001/ 3 - Meet offline at a caf√© to discuss the project more clearly and support the BE to add necessary features. - Continue focusing on the project and complete the remaining APIs. 10/11/2025 16/11/2025 http://localhost:3001/ 4 - Add warehouse (inventory) and Customer Access List. - Attach APIs to link categories and product management to the warehouse. - After linking, the ADMIN section is done. 10/11/2025 16/11/2025 http://localhost:3001/ 5 - Continue attaching APIs from the backend and test the functionalities. 10/11/2025 16/11/2025 http://localhost:3001/ 6 - Practice the workshop and write the worklog and prepare the worklog and workshop. + Create an EC2 instance + Connect via SSH + Attach an EBS volume 10/11/2025 16/11/2025 http://localhost:3001/ Week 10 Results: Completed API integration into frontend:\nAttached APIs module by module, tested using DevTools (F12). Eliminated/significantly reduced minor bugs. Organization \u0026amp; work coordination:\nAgreed on offline meeting schedule. Clarified project roadmap for upcoming milestones. Backend support \u0026amp; API completion:\nCollaborated with BE to add necessary features. Continued completing remaining APIs. Data expansion \u0026amp; administration:\nAdded warehouse (inventory) and customer access list. Linked categories \u0026amp; product management to warehouse. ADMIN section completion \u0026amp; testing:\nCompleted ADMIN module after data linking. Continued end-to-end functionality testing. Infrastructure workshop:\nCreated EC2 instance, SSH connection, attached EBS volume. Completed and updated worklog. \u0026hellip;\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Complete the basic project to prepare for submission. Complete workshop. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Code remaining features.\n- Demo and prepare for the meeting. 17/11/2025 23/11/2025 https://github.com/khanhtm45/AWS 3 - Meeting and prepare documentation for backend and frontend. - Allocate tasks reasonably, accelerate progress to meet project deadline and submit on time. 17/11/2025 23/11/2025 https://github.com/khanhtm45/AWS 4 - Complete workshop with the whole team. - Study and learn. 17/11/2025 23/11/2025 https://github.com/vanhoangkha/AWS-First-Cloud-Journey?tab=readme-ov-file 5 - Study and research applications, implement into the project if time permits. 17/11/2025 23/11/2025 https://us-east-1.console.aws.amazon.com/ec2/home?region=us-east-1 6 - Work with the team to minimize bugs as much as possible.\n- Check login \u0026amp; logout to ensure no errors occur during submission week demo. 17/11/2025 23/11/2025 https://github.com/khanhtm45/AWS Week 11 Results: Product Completion: Completed coding of the project\u0026rsquo;s missing functions and integrated components.\nQuality Assurance (QA): Reviewed and fixed minor bugs (bug fixing) with the team. Ensured important features such as Login \u0026amp; Logout worked stably, no errors occurred during the demo.\nProject Documentation: Completed drafting detailed technical documents (docx) for both Backend and Frontend.\nProgress Management: Had team meetings, divided work reasonably and promoted progress to ensure timely submission.\nLearning Activities: Completed the Workshop with the team and learned more about extended applications/services on AWS to optimize the project.\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Stop adding new features, only focus on final system review. - Check AWS connections (DB, EC2, S3\u0026hellip;) for any intermittent errors. 24/11/2025 01/12/2025 https://github.com/khanhtm45/AWS 3 - Compile documentation (Report, Slides, Source code). 24/11/2025 01/12/2025 https://github.com/khanhtm45/AWS 4 - Prepare presentation slides: Summarize architecture, technologies used and project highlights. - Divide speaking parts for each team member. 24/11/2025 01/12/2025 https://github.com/khanhtm45/AWS 5 - Run demo rehearsal: Simulate presentation to align timing and operations. - Prepare answers for potential Q\u0026amp;A questions. 24/11/2025 01/12/2025 https://github.com/khanhtm45/AWS 6 - Review AWS theoretical knowledge related to the project for exam/Q\u0026amp;A preparation. - Keep AWS server in the most stable state. 24/11/2025 01/12/2025 https://github.com/khanhtm45/AWS Week 12 Results: Completion: Full Source code, Report and Technical documentation submitted on time as required.\nDemo ready:\nAWS system thoroughly tested, running smoothly according to demo scenario. Sample data prepared and ready to demonstrate features. Presentation preparation:\nCompleted presentation slides (Presentation). Agreed on speaking script and clearly divided demo roles among members. Mentally prepared and knowledgeable for project defense. "},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/5-workshop/5.3-setup-amazon-bedrock/","title":"Setup Amazon Bedrock","tags":[],"description":"","content":"1. Go to Amazon Bedrock 2. Click on Model Catalog to find Claude Haiku 4.5 3. Click on Claude Haiku 4.5 and scroll down to find the Model ID 4. You can click \u0026ldquo;open in playground\u0026rdquo; to test the model Note: Currently you no longer need to go to model access to request model access. You can directly use the API of a model in Amazon Bedrock with an IAM account that has AWS marketplace permission once you call the model once to activate it References: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-claude.html https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 ‚Äì Guide to 6 strategies and tools for cloud cost optimization on AWS, Azure, and GCP. The article focuses on providing 6 core strategies and practical tools to optimize cloud costs systematically across AWS, Azure, and GCP. The key point is that cost optimization is a continuous and disciplined effort, starting with Right-size resources (adjust resource sizes) and clean up unused resources to avoid waste. Other important strategies include storage cost optimization by choosing the right storage tier, reducing network egress fees by managing data flows, and using each platform‚Äôs built-in optimization recommendations (such as Recommender, Trusted Advisor). The ultimate goal is to set up continuous monitoring (budget, alert) to maintain spending performance, helping organizations sustainably reduce costs, increase flexibility, and maintain confidence in their cloud strategy. With a disciplined FinOps practice, you will sustainably reduce costs, increase flexibility, and maintain leadership‚Äôs confidence in the cloud strategy.\nBlog 2 ‚Äì Seamless streaming with preconditioned ads from Olyzon and AWS Elemental MediaTailor The article focuses on a technology solution combining AWS Elemental MediaTailor and Olyzon to solve streaming interruptions caused by ads. The core idea is using preconditioned ads, allowing ads to be pre-transcoded and stitched into the content stream seamlessly, eliminating latency and buffering. Olyzon adds L-shape overlay technology to create less intrusive ads. This solution helps service providers maximize ad revenue (thanks to reliability and no missed impressions) while ensuring an optimal viewer experience.\nBlog 3 ‚Äì Register now: Updated exam and new name for the cloud operations certification The article focuses on the update and renaming of the AWS Certified SysOps Administrator ‚Äì Associate certification to AWS Certified CloudOps Engineer ‚Äì Associate (New code: SOA-C03), aiming to reflect the evolution of the Cloud Operations role and changing industry terminology. The key point is the change in the exam‚Äôs knowledge scope, including the addition of Containers, a stronger emphasis on automation, Infrastructure as Code, and operating in multi-account/multi-Region environments. This change enables professionals to demonstrate the most modern skills in deploying, operating, and maintaining workloads on AWS, thereby maximizing the credibility and relevance of the certification in the industry.\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 Event Name: GenAI Open Innovation Vietnam 2025\nDate \u0026amp; Time: 09:00, 18 September 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-powered planning, design, and coding for modern software development\nDate \u0026amp; Time: 10:00, 01 October 2025\nLocation: At home via GoToWebinar\u0026rsquo;s meeting/recording room.\nRole: Attendee\nEvent 3 Event Name: AI-Driven Development Lifecycle: Redefining Software Engineering\nTime: 14:00 ‚Äì 16:30, October 3, 2025\nLocation: AWS Event Hall, 26th Floor, Bitexco Tower, Ho Chi Minh City\nRole in the event: Attendees\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/5-workshop/5.4-setup_lambda/","title":"Setup Lambda","tags":[],"description":"","content":"1. Go to Lambda You can code with Lambda using languages as shown in the image. In this workshop, we will use Node.js 2. Click on Create function Fill in the function name and select the runtime type Click on \u0026ldquo;Change default execution role\u0026rdquo;, select \u0026ldquo;existing role\u0026rdquo;, then click on the role you created earlier for the workshop Click on \u0026ldquo;Create function\u0026rdquo; 3. Set up function\nGo to the Configuration tab Increase memory to 500MB and timeout to 2 minutes Go to the Code tab and paste this code: Click the Deploy button const { BedrockRuntimeClient, InvokeModelCommand } = require(\u0026#34;@aws-sdk/client-bedrock-runtime\u0026#34;); const client = new BedrockRuntimeClient({ region: \u0026#34;us-east-1\u0026#34; }); const CORS_HEADERS = { \u0026#34;Access-Control-Allow-Origin\u0026#34;: process.env.ALLOWED_ORIGIN || \u0026#34;*\u0026#34;, \u0026#34;Access-Control-Allow-Headers\u0026#34;: \u0026#34;Content-Type,X-Amz-Date,Authorization,X-Api-Key\u0026#34;, \u0026#34;Access-Control-Allow-Methods\u0026#34;: \u0026#34;OPTIONS,POST\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }; const MAX_MESSAGE_LENGTH = 2000; const MAX_HISTORY_TURNS = 10; exports.handler = async (event) =\u0026gt; { const startTime = Date.now(); console.log(\u0026#34;Request received:\u0026#34;, { sourceIp: event.requestContext?.identity?.sourceIp || event.requestContext?.http?.sourceIp, userAgent: event.requestContext?.identity?.userAgent || event.headers?.[\u0026#34;user-agent\u0026#34;] }); const httpMethod = event.httpMethod || event.requestContext?.http?.method; if (httpMethod === \u0026#34;OPTIONS\u0026#34;) { return { statusCode: 200, headers: CORS_HEADERS, body: \u0026#34;\u0026#34; }; } let body; try { body = JSON.parse(event.body || \u0026#34;{}\u0026#34;); } catch (e) { console.error(\u0026#34;Invalid JSON:\u0026#34;, e.message); return { statusCode: 400, headers: CORS_HEADERS, body: JSON.stringify({ error: \u0026#34;Invalid JSON format\u0026#34; }) }; } const userMessage = (body.message || \u0026#34;\u0026#34;).toString().trim(); const history = Array.isArray(body.history) ? body.history : []; if (!userMessage) { return { statusCode: 400, headers: CORS_HEADERS, body: JSON.stringify({ error: \u0026#34;Message is required\u0026#34; }) }; } if (userMessage.length \u0026gt; MAX_MESSAGE_LENGTH) { return { statusCode: 400, headers: CORS_HEADERS, body: JSON.stringify({ error: `Message too long. Maximum ${MAX_MESSAGE_LENGTH} characters allowed.` }) }; } // Claude uses messages format const messages = []; const recentHistory = history.slice(-MAX_HISTORY_TURNS); for (const turn of recentHistory) { if (turn.user) { messages.push({ role: \u0026#34;user\u0026#34;, content: turn.user }); } if (turn.assistant) { messages.push({ role: \u0026#34;assistant\u0026#34;, content: turn.assistant }); } } messages.push({ role: \u0026#34;user\u0026#34;, content: userMessage }); const requestBody = { anthropic_version: \u0026#34;bedrock-2023-05-31\u0026#34;, max_tokens: 512, messages: messages, temperature: 0.7 }; const modelId = \u0026#34;arn:aws:bedrock:us-east-1:756859458422:inference-profile/us.anthropic.claude-haiku-4-5-20251001-v1:0\u0026#34;; try { console.log(\u0026#34;Invoking Bedrock model:\u0026#34;, modelId); const command = new InvokeModelCommand({ modelId, contentType: \u0026#34;application/json\u0026#34;, accept: \u0026#34;application/json\u0026#34;, body: JSON.stringify(requestBody) }); const response = await client.send(command); const result = JSON.parse(new TextDecoder().decode(response.body)); const reply = result.content[0].text || \u0026#34;Sorry, I cannot generate a response.\u0026#34;; const duration = Date.now() - startTime; console.log(\u0026#34;Request completed:\u0026#34;, { duration, responseLength: reply.length }); return { statusCode: 200, headers: CORS_HEADERS, body: JSON.stringify({ response: reply }) }; } catch (error) { console.error(\u0026#34;Bedrock invocation error:\u0026#34;, { message: error.message, code: error.code, modelId: modelId }); return { statusCode: 500, headers: CORS_HEADERS, body: JSON.stringify({ error: \u0026#34;AI service is temporarily unavailable. Please try again later.\u0026#34; }) }; } }; ``` **Related Documentation:** https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html "},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/5-workshop/5.5-setup-api-gateway/","title":"Setup API Gateway","tags":[],"description":"","content":"1. Go to API Gateway\nClick \u0026ldquo;Create an API\u0026rdquo; Select \u0026ldquo;Build\u0026rdquo; under REST API Enter a name Choose a security policy Click \u0026ldquo;Create API\u0026rdquo; 2. Create Resource\nClick \u0026ldquo;Create Resource\u0026rdquo; Name the resource and enable CORS Click the \u0026ldquo;Create Resource\u0026rdquo; button 3. Create Method\nClick \u0026ldquo;Create Method\u0026rdquo; Select POST and enable Lambda Proxy Integration Add the Lambda function ARN Click \u0026ldquo;Create Method\u0026rdquo; 4. Deploy API\nClick \u0026ldquo;Deploy API\u0026rdquo; Choose \u0026ldquo;New Stage\u0026rdquo; and name the stage Click the \u0026ldquo;Deploy\u0026rdquo; button 5. Test\nAfter creating the stage, you will get the Invoke URL Use the Invoke URL to test in Postman with the format InvokeURL/, using the POST method Use the following JSON body:\n{ \u0026ldquo;message\u0026rdquo;: \u0026ldquo;What is Amazon Bedrock? Please answer briefly, no more than 3 lines.\u0026rdquo; }\nClick \u0026ldquo;Send\u0026rdquo; You should receive a successful response\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Overview Amazon Bedrock provides the ability to integrate leading foundation models (LLMs) from Anthropic, Meta, AI21 Labs, and other providers through a simple API, allowing you to build AI applications without managing complex infrastructure.\nIn this workshop, we will learn how to build, deploy, and test a complete AI Chatbot using a serverless architecture, enabling users to interact with Claude Haiku 4.5 without having to manage servers or worry about scaling.\nWe will create a system with five main components to build the chatbot: Amazon Bedrock (AI engine), AWS Lambda (backend logic), API Gateway (REST API endpoint), Amazon S3 (frontend hosting), and CloudWatch (monitoring). These components deliver a fully serverless architecture with low cost and automatic scaling.\nMain Components: Amazon Bedrock (AI Engine) ‚Äì Provides the Claude Haiku 4.5 model through a simple API. You call the InvokeModel API to send messages and receive intelligent AI responses.\nAWS Lambda (Backend Logic) ‚Äì Runs Node.js 24 code to process requests from API Gateway. Lambda validates input (message length, history limits), formats prompts for Bedrock, calls the Bedrock API, and handles errors.\nAPI Gateway (REST API Endpoint) ‚Äì Creates a public HTTPS endpoint (POST /chat) for the frontend to call. API Gateway handles CORS configuration, routes requests to Lambda, and provides throttling to protect the backend from abuse.\nAmazon S3 (Frontend Hosting) ‚Äì Hosts a static website (HTML) for the chatbot UI. Users access the chatbot through the browser; the interface sends messages to API Gateway and displays AI responses.\nCloudWatch (Monitoring \u0026amp; Debugging) ‚Äì Automatically collects logs from Lambda execution, allowing you to debug errors.\nArchitecture Overview User Browser ‚Üí S3 (Static Website) ‚Üí API Gateway ‚Üí Lambda ‚Üí Bedrock (Claude) ‚Üí CloudWatch\nFlow:\nUser enters a message in the chatbot UI (hosted on S3) JavaScript sends a POST request to the API Gateway endpoint API Gateway invokes the Lambda function Lambda validates input, formats the prompt, and calls the Bedrock InvokeModel API Bedrock processes it with Claude Haiku 4.5 and returns the AI response Lambda returns the response back to API Gateway ‚Üí Browser CloudWatch logs the entire process Workshop Modules Module 1: Setup Amazon Bedrock\n‚Ä¢ Enable Claude Haiku 4.5 model access\n‚Ä¢ Understand inference profiles\n‚Ä¢ Test the model via AWS Console\nModule 2: Create Lambda Function\n‚Ä¢ Create a Node.js 24 Lambda function\n‚Ä¢ Deploy chatbot backend code\n‚Ä¢ Configure IAM role with Bedrock permissions\n‚Ä¢ Set environment variables\nModule 3: Configure API Gateway\n‚Ä¢ Create REST API\n‚Ä¢ Configure POST /chat endpoint\n‚Ä¢ Enable CORS\n‚Ä¢ Test API with Postman\nModule 4: Deploy Frontend to S3\n‚Ä¢ Create S3 bucket\n‚Ä¢ Enable static website hosting\n‚Ä¢ Upload HTML chatbot UI\n‚Ä¢ Configure public access\nModule 5: Testing \u0026amp; Debugging\n‚Ä¢ Test end-to-end flow\n‚Ä¢ View CloudWatch logs\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/5-workshop/5.6-monitoring-cloudwatch/","title":"Monitoring with CloudWatch","tags":[],"description":"","content":"1. In Lambda, go to the Monitor tab\nClick \u0026ldquo;View CloudWatch logs\u0026rdquo; You can view the function logs via Log Streams "},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/5-workshop/5.7-setup-s3/","title":"Prepare S3","tags":[],"description":"","content":"1. Create an HTML file to deploy to S3\nReplace InvokeURL/resourceName in the code below with your invoke URL. \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;/\u0026gt; \u0026lt;title\u0026gt;Chatbot - ChatGPT Style\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; font-family: \u0026#34;Inter\u0026#34;, \u0026#34;Segoe UI\u0026#34;, sans-serif; background-color: #f0f0f0; display: flex; flex-direction: column; height: 100vh; } header { background-color: #2e7d32; /* Green */ color: white; padding: 1rem; text-align: center; font-weight: bold; font-size: 1.2rem; } #chatBox { flex: 1; overflow-y: auto; padding: 1.5rem; display: flex; flex-direction: column; gap: 1rem; } /* ChatGPT-like message blocks */ .message { width: 100%; max-width: 800px; margin: auto; padding: 1rem; border-radius: 10px; font-size: 1rem; line-height: 1.5; border: 1px solid #ddd; background-color: #ffffff; } .user { background-color: #e8f5e9; border-color: #c8e6c9; } .bot { background-color: #ffffff; border-color: #e0e0e0; } footer { padding: 1rem; background-color: #f7f7f8; border-top: 1px solid #ddd; display: flex; justify-content: center; } .input-container { width: 100%; max-width: 800px; display: flex; gap: 0.5rem; background: white; border: 1px solid #ccc; padding: 0.75rem; border-radius: 12px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); } input { flex: 1; border: none; outline: none; font-size: 1rem; background: none; } button { background-color: #4caf50; color: white; border: none; padding: 0.6rem 1rem; font-size: 1rem; border-radius: 8px; cursor: pointer; transition: 0.2s ease; } button:hover { background-color: #449d48; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;header\u0026gt;üåø Chatbot \u0026lt;/header\u0026gt; \u0026lt;div id=\u0026#34;chatBox\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;footer\u0026gt; \u0026lt;div class=\u0026#34;input-container\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;userInput\u0026#34; placeholder=\u0026#34;Send a message...\u0026#34; /\u0026gt; \u0026lt;button onclick=\u0026#34;sendMessage()\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;script\u0026gt; let history = []; async function sendMessage() { const userInput = document.getElementById(\u0026#34;userInput\u0026#34;); const message = userInput.value.trim(); if (!message) return; addMessage(message, \u0026#34;user\u0026#34;); const response = await fetch(\u0026#34;InvokeURL/resourceName\u0026#34;, { method: \u0026#34;POST\u0026#34;, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }, body: JSON.stringify({ message, history }) }); const data = await response.json(); const botReply = data.response; addMessage(botReply, \u0026#34;bot\u0026#34;); history.push({ user: message, assistant: botReply }); userInput.value = \u0026#34;\u0026#34;; } function addMessage(text, sender) { const msg = document.createElement(\u0026#34;div\u0026#34;); msg.classList.add(\u0026#34;message\u0026#34;, sender); msg.textContent = text; document.getElementById(\u0026#34;chatBox\u0026#34;).appendChild(msg); msg.scrollIntoView({ behavior: \u0026#34;smooth\u0026#34; }); } \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 2. Go to S3\nClick Create bucket\nEnter a bucket name\nDisable Block public access\n(While the best practice is keeping the bucket private, for this workshop we set it to public for easier testing.)\nClick Create bucket\n3. Open the bucket you just created\nGo to the Properties tab\nClick Edit under Static website hosting\nSelect Enable\nClick Save changes\nGo to the Permissions tab ‚Üí Edit Bucket policy\nPaste the following policy (replace your-bucket-name with your actual bucket name):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::your-bucket-name/*\u0026#34; } ] } Click Save changes\nGo to the Objects tab and upload your HTML file\nClick the Object URL to open your deployed HTML page\nDeployment completed successfully\n"},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://lebao004.github.io/AWS_Worklog_2025/tags/","title":"Tags","tags":[],"description":"","content":""}]